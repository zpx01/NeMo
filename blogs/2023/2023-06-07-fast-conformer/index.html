<!doctype html><html lang=en class=no-js> <head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="Efficient training and inference on long audio with Fast Conformer architecture"><meta name=author content="['Dima Rekesh', 'Samuel Kriman', 'Somshubra Majumdar', 'Vahid Noroozi', 'He Huang', 'Oleksii Hrinchuk', 'Ankur Kumar', 'Boris Ginsburg']"><link href=https://nvidia.github.io/NeMo/blogs/2023/2023-06-07-fast-conformer/ rel=canonical><link href=../2023-nvidia-technical-blog/ rel=prev><link href=../2023-08-nfa/ rel=next><link rel=icon href=../../../assets/favicon.png><meta name=generator content="mkdocs-1.5.3, mkdocs-material-9.5.6"><title>Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition - NVIDIA NeMo</title><link rel=stylesheet href=../../../assets/stylesheets/main.50c56a3b.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.06af60db.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../assets/stylesheets/extra.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><!-- Add scripts that need to run before here --><!-- Add scripts that need to run afterward here --><!-- Add Twitter Card metadata --><meta name=twitter:card content=summary_large_image><!-- Add OpenGraph Title metadata --><meta property=og:title content="Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition"><meta name=twitter:title content="Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition"><!-- Add OpenGraph Type metadata --><meta property=og:type content=website><!-- Add OpenGraph URL metadata --><meta content=https://nvidia.github.io/NeMo/blogs/2023/2023-06-07-fast-conformer/ property=og:url><!-- Add OpenGraph Image metadata --><meta property=og:image content=https://github.com/NVIDIA/NeMo/releases/download/v1.18.0/asset-post-fast-conformer-diagram.png><meta property=twitter:image content=https://github.com/NVIDIA/NeMo/releases/download/v1.18.0/asset-post-fast-conformer-diagram.png><!-- Add OpenGraph Image Type metadata --><meta property=og:image:type content=image/png><!-- Add OpenGraph Description metadata --><meta property=og:description content="Efficient training and inference on long audio with Fast Conformer architecture"><meta property=twitter:description content="Efficient training and inference on long audio with Fast Conformer architecture"></head> <body dir=ltr data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#fast-conformer-with-linearly-scalable-attention-for-efficient-speech-recognition class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class="md-header md-header--shadow md-header--lifted" data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="NVIDIA NeMo" class="md-header__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> NVIDIA NeMo </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media data-md-color-scheme=midnight-black data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to dark mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="Switch to dark mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> <input class=md-option data-md-color-media data-md-color-scheme=slate data-md-color-primary=indigo data-md-color-accent=light-blue aria-label="Switch to light mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="Switch to light mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12c0-2.42-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12 20 8.69Z"/></svg> </label> </form> <script>var media,input,key,value,palette=__md_get("__palette");if(palette&&palette.color){"(prefers-color-scheme)"===palette.color.media&&(media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']"),palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent"));for([key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7 0-.24-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91 1.61 0 2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08Z"/></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> </nav> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> Home </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../ class=md-tabs__link> Blog </a> </li> <li class=md-tabs__item> <a href=../../../publications/ class=md-tabs__link> Publications </a> </li> </ul> </div> </nav> </header> <div class=md-container data-md-component=container> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation hidden> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="NVIDIA NeMo" class="md-nav__button md-logo" aria-label="NVIDIA NeMo" data-md-component=logo> <svg id=Layer_1 data-name="Layer 1" xmlns=http://www.w3.org/2000/svg viewbox="0 0 16 16"> <defs> <style>
      .cls-1 {
        fill: #76b900;
      }
    </style> </defs> <g id=NVIDIA_Logo data-name="NVIDIA Logo"> <path id=Eye_Mark class=cls-1 d=M5.9698,5.86543V4.90907c.09285-.00661.18663-.01156.28219-.01456A5.75152,5.75152,0,0,1,10.58372,7.142S8.73032,9.71631,6.74309,9.71631a2.40975,2.40975,0,0,1-.77329-.12364v-2.9c1.01832.123,1.223.57279,1.83539,1.59325l1.36157-1.148A3.60517,3.60517,0,0,0,6.49742,5.83431a4.93745,4.93745,0,0,0-.52762.03112m0-3.15922V4.13474c.09389-.00742.1879-.0134.28219-.0168,3.63754-.12254,6.0073,2.98317,6.0073,2.98317s-2.722,3.31-5.55774,3.31a4.18488,4.18488,0,0,1-.73175-.06444v.883a4.81728,4.81728,0,0,0,.60938.03947c2.639,0,4.54736-1.34759,6.39542-2.94267.30618.24532,1.56062.8421,1.8186,1.1037-1.75722,1.47088-5.852,2.65644-8.17346,2.65644-.22369,0-.43886-.01352-.64994-.03376v1.241H16V2.70621Zm0,6.88646v.754A4.26109,4.26109,0,0,1,2.85159,7.37428,5.27645,5.27645,0,0,1,5.9698,5.86543v.8272l-.0038-.0004a2.34214,2.34214,0,0,0-1.81935.83163A3.25091,3.25091,0,0,0,5.9698,9.59267M1.63473,7.26433A6.045,6.045,0,0,1,5.9698,4.90907V4.13474C2.77053,4.39151,0,7.10111,0,7.10111s1.56908,4.53638,5.9698,4.95171v-.82318C2.74044,10.82334,1.63473,7.26433,1.63473,7.26433Z data-name="Eye Mark"/> </g> </svg> </a> NVIDIA NeMo </label> <div class=md-nav__source> <a href=https://github.com/NVIDIA/NeMo title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M439.55 236.05 244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg> </div> <div class=md-source__repository> NVIDIA/NeMo </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2 checked> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <span class=md-ellipsis> Blog </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=true> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Blog </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../archive/2024/ class=md-nav__link> <span class=md-ellipsis> 2024 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../category/announcements/ class=md-nav__link> <span class=md-ellipsis> Announcements </span> </a> </li> <li class=md-nav__item> <a href=../../category/nvidia-technical-blog/ class=md-nav__link> <span class=md-ellipsis> NVIDIA Technical blog </span> </a> </li> <li class=md-nav__item> <a href=../../category/papers/ class=md-nav__link> <span class=md-ellipsis> Papers </span> </a> </li> <li class=md-nav__item> <a href=../../category/technical-deep-dive/ class=md-nav__link> <span class=md-ellipsis> Technical deep-dive </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../../publications/ class="md-nav__link "> <span class=md-ellipsis> Publications </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> Publications </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex> <span class=md-ellipsis> Archive </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Archive </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../publications/archive/2023/ class=md-nav__link> <span class=md-ellipsis> 2023 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2022/ class=md-nav__link> <span class=md-ellipsis> 2022 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2021/ class=md-nav__link> <span class=md-ellipsis> 2021 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2020/ class=md-nav__link> <span class=md-ellipsis> 2020 </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/archive/2019/ class=md-nav__link> <span class=md-ellipsis> 2019 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex> <span class=md-ellipsis> Categories </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Categories </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../publications/category/inverse-text-normalization/ class=md-nav__link> <span class=md-ellipsis> (Inverse) Text Normalization </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/automatic-speech-recognition/ class=md-nav__link> <span class=md-ellipsis> Automatic Speech Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/dialog-state-tracking/ class=md-nav__link> <span class=md-ellipsis> Dialog State Tracking </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/large-language-models/ class=md-nav__link> <span class=md-ellipsis> Large Language Models </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/natural-language-processing/ class=md-nav__link> <span class=md-ellipsis> Natural Language Processing </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/neural-machine-translation/ class=md-nav__link> <span class=md-ellipsis> Neural Machine Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speaker-recognition/ class=md-nav__link> <span class=md-ellipsis> Speaker Recognition </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speech-classification/ class=md-nav__link> <span class=md-ellipsis> Speech Classification </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/speech-translation/ class=md-nav__link> <span class=md-ellipsis> Speech Translation </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/text-to-speech/ class=md-nav__link> <span class=md-ellipsis> Text to Speech </span> </a> </li> <li class=md-nav__item> <a href=../../../publications/category/tools/ class=md-nav__link> <span class=md-ellipsis> Tools </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#fast-conformer-architecture class=md-nav__link> <span class=md-ellipsis> Fast Conformer: Architecture </span> </a> </li> <li class=md-nav__item> <a href=#fast-conformer-linearly-scalable-attention class=md-nav__link> <span class=md-ellipsis> Fast Conformer: Linearly Scalable Attention </span> </a> </li> <li class=md-nav__item> <a href=#checkpoints class=md-nav__link> <span class=md-ellipsis> Checkpoints </span> </a> </li> <li class=md-nav__item> <a href=#data-processing class=md-nav__link> <span class=md-ellipsis> Data Processing </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <article class="md-content__inner md-typeset"> <!-- Edit button --> <a href=https://github.com/NVIDIA/NeMo/edit/master/docs/blogs/posts/2023/2023-06-07-fast-conformer.md title=edit.link.title class="md-content__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25Z"/></svg> </a> <!-- Back to index button --> <!-- Note: using "align-items:center" to make sure that the arrow looks vertically centered relative to the text --> <a href=../../ class=" md-nav__link" style="align-items:center; color: var(--md-default-fg-color--light); padding-bottom:20px; font-weight: 700;"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> <span class=md-ellipsis> Back to index </span> </a> <!-- Blogging Markup --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>Dima Rekesh</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/bmwshop>@bmwshop</a> <!-- Add comma if not last author --> , </span> <!-- If number of authors is less than 3, put on new lines --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>Samuel Kriman</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/sam1373>@sam1373</a> <!-- Add comma if not last author --> , </span> <!-- If number of authors is less than 3, put on new lines --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>Somshubra Majumdar</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/titu1994>@titu1994</a> <!-- Add comma if not last author --> , </span> <!-- If number of authors is less than 3, put on new lines --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>Vahid Noroozi</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/VahidooX>@VahidooX</a> <!-- Add comma if not last author --> , </span> <!-- If number of authors is less than 3, put on new lines --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>He Huang</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/stevehuang52>@stevehuang52</a> <!-- Add comma if not last author --> , </span> <!-- If number of authors is less than 3, put on new lines --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>Oleksii Hrinchuk</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/AlexGrinch>@AlexGrinch</a> <!-- Add comma if not last author --> , </span> <!-- If number of authors is less than 3, put on new lines --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>Ankur Kumar</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/iankur>@iankur</a> <!-- Add comma if not last author --> , </span> <!-- If number of authors is less than 3, put on new lines --> <!--        <p class="p-blog">--> <!--        <img src="https://avatars.githubusercontent.com/" alt="@">--> <!--        </p>--> <!--        <p class="p-blog">--> <!-- If number of authors of less than 3, put on new lines --> <!-- Add author name --> <span class=post-author> <b>Boris Ginsburg</b> <!-- Add author GitHub link if 1:1 map exists between author names and author gh ids --> · <a href=https://github.com/borisgin>@borisgin</a> <!-- Add comma if not last author --> </span> <!-- If number of authors is less than 3, put on new lines --> <p class=p-blog> <span> <!-- Note: using "margin-bottom:-2px" to make sure icons look more-or-less vertically centered relative to the text --> <svg style=margin-bottom:-2px; xmlns=http://www.w3.org/2000/svg width=16 height=16 fill=currentColor class="bi bi-calendar2" viewbox="0 0 16 16"> <path d="M3.5 0a.5.5 0 0 1 .5.5V1h8V.5a.5.5 0 0 1 1 0V1h1a2 2 0 0 1 2 2v11a2 2 0 0 1-2 2H2a2 2 0 0 1-2-2V3a2 2 0 0 1 2-2h1V.5a.5.5 0 0 1 .5-.5zM2 2a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h12a1 1 0 0 0 1-1V3a1 1 0 0 0-1-1H2z"/> <path d="M2.5 4a.5.5 0 0 1 .5-.5h10a.5.5 0 0 1 .5.5v1a.5.5 0 0 1-.5.5H3a.5.5 0 0 1-.5-.5V4z"/> </svg> 2023-06-07 · <svg style=margin-bottom:-2px; xmlns=http://www.w3.org/2000/svg width=16 height=16 fill=currentColor class="bi bi-clock" viewbox="0 0 16 16"> <path d="M8 3.5a.5.5 0 0 0-1 0V9a.5.5 0 0 0 .252.434l3.5 2a.5.5 0 0 0 .496-.868L8 8.71V3.5z"/> <path d="M8 16A8 8 0 1 0 8 0a8 8 0 0 0 0 16zm7-8A7 7 0 1 1 1 8a7 7 0 0 1 14 0z"/> </svg> 10 minute read </span> </p> <!-- Markdown content --> <h1 id=fast-conformer-with-linearly-scalable-attention-for-efficient-speech-recognition>Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition<a class=headerlink href=#fast-conformer-with-linearly-scalable-attention-for-efficient-speech-recognition title="Permanent link">&para;</a></h1> <p>The Conformer architecture, introduced by <a href=https://arxiv.org/abs/2005.08100>Gulati et al.</a> has been a standard architecture used for not only Automatic Speech Recognition, but has also been extended to other tasks such as Spoken Language Understanding, Speech Translation, and used as a backbone for Self Supervised Learning for various downstream tasks. While they are highly accurate models on each of these tasks, and can be extended for use in other tasks, they are also very computationally expensive. This is due to the quadratic complexity of the attention mechanism, which makes it difficult to train and infer on long sequences, which are used as input to these models due to the granular stride of audio pre-processors (commonly Mel Spectrograms or even raw audio signal in certain models with 10 milliseconds stride). Furthermore, the memory requirement of quadratic attention also significantly limits the audio duration during inference.</p> <!-- more --> <p>In this paper, we introduce the <strong>Fast Conformer</strong> architecture, which applies simple changes to the architecture to significantly reduce the computational cost of training and inference, all while mantaining the strong results of the original Conformer model. We further show that by modifying (on the fly) the global attention module to a linearly scalable attention mechanism - the same model can be used to train (or finetune) and then infer on long sequences (up to 1 hour !).</p> <p>Please refer to the paper here: <a href=https://arxiv.org/abs/2305.05084>Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition</a></p> <div align=center> <img src=https://github.com/NVIDIA/NeMo/releases/download/v1.18.0/asset-post-fast-conformer-diagram.png> </div> <h2 id=fast-conformer-architecture>Fast Conformer: Architecture<a class=headerlink href=#fast-conformer-architecture title="Permanent link">&para;</a></h2> <p>We propose 4 changes to the original Conformer architecture to make it more efficient:</p> <p>1) <strong>Downsampling module</strong>: The original Conformer paper uses a stack of 2-D Convolutions with a large number of output filters to perform the downsampling in order to reduce the resolution of the incoming audio frames from 10 ms to 40 ms, which makes it more tractable for the subsequent attention layers to operate on. However, these convolutions amount to roughly 20 % of the entire time required to perform a single forward pass of the "Large" Conformer (with 120 M parameters). Furthermore, due to the quadratic attention cost, we can obtain a 4x reduction in all subsequent attention layers by downsampling the audio to 80 ms frames. So as the first change, we perform 8x downsampling before any of the Conformer layers are applied. This reduces GMACs to roughly 65% of the baseline Conformer.</p> <p>2) <strong>Depthwise convolution</strong>: Multiple other works have shown that it is not necessary to use full Convolution layers and that we can save both compute and memory simply by using Depthwise Separable Convoltions. Therefore, we replace all Convolution layers in the preprocessor by Depthwise Seperable Convolutions. This reduces GMACs to roughly 37% of the baseline Conformer.</p> <p>3) <strong>Channel reduction</strong>: In literature, it is common for the hidden dimension of the downsampling module to match the <code>d_model</code> of the Conformer module for easy application to the subsequent stack of Conformer modules. However, this is not necessary, and we can reduce the number of channels in the downsampling module to reduce the number of parameters and GMACs. We reduce the number of channels to 256, which reduces GMACs to roughly 33% of the baseline Conformer.</p> <p>4) <strong>Kernel size reduction</strong>: Finally, as we have performed 8x downsampling of the incoming audio, it is no longer required to use the rather large kernel size of 31 in the Convolution layers of the Conformer block. We find that we can roughly reduce it to 9, which preserves the same accuracy while executing slightly faster and reducing the memory cost. This finally reduces GMACs to roughly 33% of the baseline Conformer.</p> <p><em>Below, we tabulate the accuracy vs speed for each component of Fast Conformer modification schema. Models were tested on LibriSpeech test-other incrementally for each modification starting from the original Conformer. Encoder inference speed (samples/sec) was measured with batch size 128 on 20 sec audio samples. The number of parameters (M) is shown for the encoder only.</em> </p> <table> <thead> <tr> <th>Encoder</th> <th>WER, Test Other %</th> <th>Inference Samples / sec</th> <th>Params (M)</th> <th>GMACS</th> </tr> </thead> <tbody> <tr> <td>Baseline Conformer</td> <td>5.19</td> <td>169</td> <td>115</td> <td>143.2</td> </tr> <tr> <td> +8X Stride</td> <td>5.11</td> <td>303</td> <td>115</td> <td>92.5</td> </tr> <tr> <td> +Depthwise conv</td> <td>5.12</td> <td>344</td> <td>111</td> <td>53.2</td> </tr> <tr> <td> +256 channels</td> <td>5.09</td> <td>397</td> <td>109</td> <td>48.8</td> </tr> <tr> <td> +Kernel 9</td> <td>4.99</td> <td>467</td> <td>109</td> <td>48.7</td> </tr> </tbody> </table> <h2 id=fast-conformer-linearly-scalable-attention>Fast Conformer: Linearly Scalable Attention<a class=headerlink href=#fast-conformer-linearly-scalable-attention title="Permanent link">&para;</a></h2> <p>On an NVIDIA A100 GPU with 80 GB of memory, we find that Conformer reaches the memory limit at around 10-minute long single audio clip. This mean that it is not feasible to perform inference without performing streaming inference which may lead to degraded results.</p> <p>Fast Conformer, due to its 8x stride fairs a little better and can perform inference on roughly 18-minute long audio clips. However, this is still not sufficient for many use cases.</p> <p>We therefore extend <a href=https://arxiv.org/abs/2004.05150>Longformer</a> to the Conformer architecture. Longformer uses local attention augmented with global tokens. We use a single global attention token, which attends to all other tokens and has all other tokens attend to it, using a separate set of query, key and value linear projections, while others attend in a fixed-size window surrounding each token (see below). </p> <div align=center> <img src=https://github.com/NVIDIA/NeMo/releases/download/v1.18.0/asset-post-fast-conformer-local-attn.png width=100%> </div> <p>By switching to limited context attention, we extend the maximum duration that the model can process at once on a single A100 GPU by ~4x: from 10 minutes for Conformer to 18 minutes for Fast Conformer. Furthermore, you can use a pre-trained Fast Conformer model and readily convert its attention to Longformer attention without any further training ! While this will not use the global attention token, it will still be able to process 70-minute long audio clips.`</p> <h2 id=checkpoints>Checkpoints<a class=headerlink href=#checkpoints title="Permanent link">&para;</a></h2> <p>We provide checkpoints for multiple languages on <a href="https://huggingface.co/models?sort=downloads&search=fastconformer">HuggingFace</a> and will add more when we support other languages or tasks.</p> <p>Each of these models are "Punctuation and Capitalization" enabled - meaning that they can be used to perform ASR and Punctuation and Capitalization (PnC) in a single pass and can produce text that is more natural to read. Post-processing to normalize text will be provided in a future release.</p> <p>Some languages we currently support for ASR are :</p> <ul> <li><a href=https://huggingface.co/nvidia/stt_en_fastconformer_hybrid_large_pc>English</a></li> <li><a href=https://huggingface.co/nvidia/stt_fr_fastconformer_hybrid_large_pc>French</a></li> <li><a href=https://huggingface.co/nvidia/stt_de_fastconformer_hybrid_large_pc>German</a></li> <li><a href=https://huggingface.co/nvidia/stt_it_fastconformer_hybrid_large_pc>Italian</a></li> <li><a href=https://huggingface.co/nvidia/stt_es_fastconformer_hybrid_large_pc>Spanish</a></li> <li><a href=https://huggingface.co/nvidia/stt_be_fastconformer_hybrid_large_pc>Belarusian</a></li> <li><a href=https://huggingface.co/nvidia/stt_hr_fastconformer_hybrid_large_pc>Croatian</a></li> <li><a href=https://huggingface.co/nvidia/stt_pl_fastconformer_hybrid_large_pc>Polish</a></li> <li><a href=https://huggingface.co/nvidia/stt_ua_fastconformer_hybrid_large_pc>Ukranian</a></li> <li><a href=https://huggingface.co/nvidia/stt_ru_fastconformer_hybrid_large_pc>Russian</a></li> </ul> <h2 id=data-processing>Data Processing<a class=headerlink href=#data-processing title="Permanent link">&para;</a></h2> <p>When constructing datasets with support for Punctuation and Capitalization, dataset preparation is an important piece of the puzzle. When training a model with Punctuation and Capitalization, you may face the following issues:</p> <p>1) During training, there may be the case the standard evaluation benchmarks do not have punctuation or capitalization, but the model still predicts them, providing an incorrect evaluation of model training. </p> <p>2) Not all training data may have punctuation or capitalization, so you may want to filter out such samples to prevent confusing the model about whether it should predict punctuation or capitalization.</p> <p>In order to provide a consistent and reproducible way to process such dataset, we will begin providing the dataset preprocessing strategies in <a href=https://github.com/NVIDIA/NeMo-speech-data-processor>Speech Data Processor</a>.</p> <p>Speech Dataset Processor currently hosts dataset processing recipies for Spanish, and we will add more languages in the future.</p> <h1 id=usage>Usage<a class=headerlink href=#usage title="Permanent link">&para;</a></h1> <p>Fast Conformer can be instantiated and used with just a few lines of code when the NeMo ASR library is installed.</p> <h2 id=global-attention>Global Attention<a class=headerlink href=#global-attention title="Permanent link">&para;</a></h2> <p>For global attention on modest files (upto 15-18 minutes on an A100), you can perform the following steps :</p> <div class=highlight><pre><span></span><code><a id=__codelineno-0-1 name=__codelineno-0-1 href=#__codelineno-0-1></a><span class=kn>from</span> <span class=nn>nemo.collections.asr.models</span> <span class=kn>import</span> <span class=n>ASRModel</span>
<a id=__codelineno-0-2 name=__codelineno-0-2 href=#__codelineno-0-2></a>
<a id=__codelineno-0-3 name=__codelineno-0-3 href=#__codelineno-0-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>ASRModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;nvidia/stt_en_fastconformer_hybrid_large_pc&quot;</span><span class=p>)</span>
<a id=__codelineno-0-4 name=__codelineno-0-4 href=#__codelineno-0-4></a><span class=n>model</span><span class=o>.</span><span class=n>transcribe</span><span class=p>([</span><span class=s2>&quot;&lt;path to a audio file&gt;.wav&quot;</span><span class=p>])</span>  <span class=c1># ~10-15 minutes long!</span>
</code></pre></div> <h2 id=local-attention>Local Attention<a class=headerlink href=#local-attention title="Permanent link">&para;</a></h2> <p>Coming in NeMo 1.20, you can easily modify the attention type to local attention after building the model. Then you can also apply audio chunking for the subsampling module to perform inference on huge audio files!</p> <p>For local attention on huge files (upto 11 hours on an A100), you can perform the following steps :</p> <div class=highlight><pre><span></span><code><a id=__codelineno-1-1 name=__codelineno-1-1 href=#__codelineno-1-1></a><span class=kn>from</span> <span class=nn>nemo.collections.asr.models</span> <span class=kn>import</span> <span class=n>ASRModel</span>
<a id=__codelineno-1-2 name=__codelineno-1-2 href=#__codelineno-1-2></a>
<a id=__codelineno-1-3 name=__codelineno-1-3 href=#__codelineno-1-3></a><span class=n>model</span> <span class=o>=</span> <span class=n>ASRModel</span><span class=o>.</span><span class=n>from_pretrained</span><span class=p>(</span><span class=s2>&quot;nvidia/stt_en_fastconformer_hybrid_large_pc&quot;</span><span class=p>)</span>
<a id=__codelineno-1-4 name=__codelineno-1-4 href=#__codelineno-1-4></a>
<a id=__codelineno-1-5 name=__codelineno-1-5 href=#__codelineno-1-5></a><span class=n>model</span><span class=o>.</span><span class=n>change_attention_model</span><span class=p>(</span><span class=s2>&quot;rel_pos_local_attn&quot;</span><span class=p>,</span> <span class=p>[</span><span class=mi>128</span><span class=p>,</span> <span class=mi>128</span><span class=p>])</span>  <span class=c1># local attn</span>
<a id=__codelineno-1-6 name=__codelineno-1-6 href=#__codelineno-1-6></a>
<a id=__codelineno-1-7 name=__codelineno-1-7 href=#__codelineno-1-7></a><span class=c1># (Coming in NeMo 1.20)</span>
<a id=__codelineno-1-8 name=__codelineno-1-8 href=#__codelineno-1-8></a><span class=n>model</span><span class=o>.</span><span class=n>change_subsampling_conv_chunking_factor</span><span class=p>(</span><span class=mi>1</span><span class=p>)</span>  <span class=c1># 1 = auto select</span>
<a id=__codelineno-1-9 name=__codelineno-1-9 href=#__codelineno-1-9></a>
<a id=__codelineno-1-10 name=__codelineno-1-10 href=#__codelineno-1-10></a><span class=n>model</span><span class=o>.</span><span class=n>transcribe</span><span class=p>([</span><span class=s2>&quot;&lt;path to a huge audio file&gt;.wav&quot;</span><span class=p>])</span>  <span class=c1># 10+ hours !</span>
</code></pre></div> <h1 id=results>Results<a class=headerlink href=#results title="Permanent link">&para;</a></h1> <p>By performing the simple modifications in Fast Conformer, we obtain strong scores throughout multiple speech tasks as shown below, all while having more efficient training and inference.</p> <p>For ASR alone, we obtain a 2.8x speedup as compared to Conformer encoder of similar size for inference, and can use larger batch sizes during training to further speedup training. We also compare results against tasks such as Speech Translation and Spoken Language Understanding in order to validate that these changes lead only to improvements in efficiency and do not degrade accuracy on downstream tasks.</p> <h2 id=asr-results>ASR Results<a class=headerlink href=#asr-results title="Permanent link">&para;</a></h2> <p>Below, we list some of our results on Fast Conformer on LibriSpeech test-other. We compare against the original Conformer and other recent efficient architectures. We compare against the <a href=https://arxiv.org/abs/2109.01163>Efficient Conformer from Burchi2021</a> which uses a progressive downsampling of the Conformer architecture. We also compare against <a href=https://arxiv.org/abs/2206.00888>Kim2022 SqueezeFormer</a> which uses a U-Net like architecture to progressively downsample the input and upsample it to 40 ms resolution prior to applying the decoder. </p> <p>We find that Fast Conformer is able to achieve the same accuracy as the Conformer while being 2.8x faster and using fewer parameters. </p> <p><em>Fast Conformer-Large with CTC and RNNT decoders trained on Librispeech. Greedy WER (%) was measured on Librispeech test-other. The number of parameters (M) and compute (Multiply-Acc, GMAC) are shown for encoder only.</em></p> <table> <thead> <tr> <th>Encoder</th> <th style="text-align: center;">WER, %</th> <th style="text-align: center;">Params,</th> <th style="text-align: center;">Compute,</th> </tr> </thead> <tbody> <tr> <td></td> <td style="text-align: center;">test-other</td> <td style="text-align: center;">M</td> <td style="text-align: center;">GMACS</td> </tr> <tr> <td><strong>RNNT decoder</strong></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> </tr> <tr> <td>Conformer</td> <td style="text-align: center;">5.19</td> <td style="text-align: center;">115</td> <td style="text-align: center;">143.2</td> </tr> <tr> <td>Fast Conformer</td> <td style="text-align: center;">4.99</td> <td style="text-align: center;">109</td> <td style="text-align: center;">48.7</td> </tr> <tr> <td><strong>CTC decoder</strong></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> </tr> <tr> <td>Conformer</td> <td style="text-align: center;">5.74</td> <td style="text-align: center;">121</td> <td style="text-align: center;">149.2</td> </tr> <tr> <td>Eff. Conformer [Burchi2021]</td> <td style="text-align: center;">5.79</td> <td style="text-align: center;">125</td> <td style="text-align: center;">101.3</td> </tr> <tr> <td>SqeezeFormer [Kim2022]</td> <td style="text-align: center;">6.05</td> <td style="text-align: center;">125</td> <td style="text-align: center;">91.0</td> </tr> <tr> <td>Fast Conformer</td> <td style="text-align: center;">6.00</td> <td style="text-align: center;">115</td> <td style="text-align: center;">51.5</td> </tr> </tbody> </table> <h2 id=speech-translation-results>Speech Translation Results<a class=headerlink href=#speech-translation-results title="Permanent link">&para;</a></h2> <p>We also evaluate Fast Conformer on the IWSLT 2014 German-English speech translation task. We find that Fast Conformer is able to achieve the same accuracy as the Conformer while being upto 1.8x faster and using fewer parameters.</p> <p>Our models have been trained on all available datasets from IWSLT22 competition which corresponds to 4k hours of speech. Some of the datasets did not contain German translations, so we generated them ourselves with text-to-text machine translation model trained on WMT21 and in-domain finetuned on Must-C v2.</p> <p><em>Speech Translation, MUST-C V2 tst-COMMON dataset. SacreBLEU, total inference time, and relative inference speed-up were measured with a batch size <span class=arithmatex>\(32\)</span> for two speech translation models with Conformer-based encoder and either RNNT, or Transformer decoder.</em></p> <table> <thead> <tr> <th>Encoder</th> <th style="text-align: center;">BLEU</th> <th style="text-align: center;">Time (sec)</th> <th style="text-align: center;">Speed-up</th> </tr> </thead> <tbody> <tr> <td><strong>Transformer decoder</strong></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> </tr> <tr> <td>Conformer</td> <td style="text-align: center;">31.0</td> <td style="text-align: center;">267</td> <td style="text-align: center;">1X</td> </tr> <tr> <td>Fast Conformer</td> <td style="text-align: center;">31.4</td> <td style="text-align: center;">161</td> <td style="text-align: center;">1.66X</td> </tr> <tr> <td><strong>RNNT decoder</strong></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> </tr> <tr> <td>Conformer</td> <td style="text-align: center;">23.2</td> <td style="text-align: center;">83</td> <td style="text-align: center;">1X</td> </tr> <tr> <td>Fast Conformer</td> <td style="text-align: center;">27.9</td> <td style="text-align: center;">45</td> <td style="text-align: center;">1.84X</td> </tr> </tbody> </table> <h2 id=spoken-language-understanding-results>Spoken Language Understanding Results<a class=headerlink href=#spoken-language-understanding-results title="Permanent link">&para;</a></h2> <p>For the Speech Intent Clasification and Slot Filling task, experiments are conducted using the <a href=https://arxiv.org/abs/2011.13205>SLURP</a> dataset, where intent accuracy and SLURP-F1 are used as the evaluation metric. For a fair comparison, both Conformer and Fast Conformer models are initialized by training on the same NeMo ASR Set dataset (roughly 25,000 hours of speech) and then the weights of the entire model are finetuned with the respective decoders.</p> <p><em>Speech intent classification and slot filling on SLURP dataset. ESPNet-SLU and SpeechBrain-SLU models use a <a href=https://arxiv.org/abs/2106.07447>HuBERT</a> encoder pre-trained via a self-supervised objective on <a href=https://arxiv.org/abs/1912.07875>LibriLight-60k</a>. Inference time and relative speed-up against Conformer are measured with batch size 32.</em></p> <table> <thead> <tr> <th>Model</th> <th style="text-align: center;">Intent Acc</th> <th style="text-align: center;">SLURP F1</th> <th style="text-align: center;">Inference, sec</th> <th style="text-align: center;">Rel. Speed-up</th> </tr> </thead> <tbody> <tr> <td>SpeechBrain-SLU</td> <td style="text-align: center;">87.70</td> <td style="text-align: center;">76.19</td> <td style="text-align: center;">-</td> <td style="text-align: center;">-</td> </tr> <tr> <td>ESPnet-SLU</td> <td style="text-align: center;">86.52</td> <td style="text-align: center;">76.91</td> <td style="text-align: center;">-</td> <td style="text-align: center;">-</td> </tr> <tr> <td><strong>Conformer/Fast Conformer+Transformer Decoder</strong></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> <td style="text-align: center;"></td> </tr> <tr> <td>Conformer</td> <td style="text-align: center;">90.14</td> <td style="text-align: center;">82.27</td> <td style="text-align: center;">210</td> <td style="text-align: center;">1X</td> </tr> <tr> <td>Fast Conformer</td> <td style="text-align: center;">90.68</td> <td style="text-align: center;">82.04</td> <td style="text-align: center;">191</td> <td style="text-align: center;">1.1X</td> </tr> </tbody> </table> <h2 id=long-form-speech-recognition-results>Long Form Speech Recognition Results<a class=headerlink href=#long-form-speech-recognition-results title="Permanent link">&para;</a></h2> <p>While Fast Conformer can be modified post training to do simple inference on long audio, due to the mismatch in attention window with limited future information, the model's WER may degrade a small amount. Users can therefore add global token followed by subsequent fine-tuning for some small steps on the same dataset in order to significantly recover (and outperform) the original models WER.</p> <p>Note that with Longformer style attention, we can still perform buffered inference with large chunk size - upto 1 hour long, therefore inference on even longer audio can be done efficiently with few inference steps.</p> <p><em>Fast Conformer versus Conformer on long audio. We evaluated four versions of FastConformer: (1) FC with global attention (2) FC with limited context (3) FC with limited context and global token (4) FC with limited context and global token, trained on long concatenated utterances. Models have been evaluated on two long-audio bencmarks: TED-LIUM v3 and Earning 21. Greedy WER(%).</em></p> <table> <thead> <tr> <th>Model</th> <th style="text-align: center;">TED-LIUM v3</th> <th style="text-align: center;">Earnings21</th> </tr> </thead> <tbody> <tr> <td>Conformer</td> <td style="text-align: center;">9.71</td> <td style="text-align: center;">24.34</td> </tr> <tr> <td>Fast Conformer</td> <td style="text-align: center;">9.85</td> <td style="text-align: center;">23.84</td> </tr> <tr> <td> + Limited Context</td> <td style="text-align: center;">9.92</td> <td style="text-align: center;">28.42</td> </tr> <tr> <td> + Global Token</td> <td style="text-align: center;">8.00</td> <td style="text-align: center;">20.68</td> </tr> <tr> <td> + Concat utterances</td> <td style="text-align: center;">7.85</td> <td style="text-align: center;">19.52</td> </tr> </tbody> </table> <!-- Last update of source file --> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var tab,labels=set.querySelector(".tabbed-labels");for(tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8v12Z"/></svg> Back to top </button> </main> <footer class=md-footer> <nav class="md-footer__inner md-grid" aria-label=Footer> <a href=../2023-nvidia-technical-blog/ class="md-footer__link md-footer__link--prev" aria-label="Previous: NVIDIA Technical Blog 2023"> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg> </div> <div class=md-footer__title> <span class=md-footer__direction> Previous </span> <div class=md-ellipsis> NVIDIA Technical Blog 2023 </div> </div> </a> <a href=../2023-08-nfa/ class="md-footer__link md-footer__link--next" aria-label="Next: Introducing NeMo Forced Aligner"> <div class=md-footer__title> <span class=md-footer__direction> Next </span> <div class=md-ellipsis> Introducing NeMo Forced Aligner </div> </div> <div class="md-footer__button md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-copyright> <div class=md-copyright__highlight> Copyright &copy; 2019 - 2023 NVIDIA </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <div class=md-social> <a href=https://github.com/NVIDIA/NeMo target=_blank rel=noopener title=github.com class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 496 512"><!-- Font Awesome Free 6.5.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2023 Fonticons, Inc.--><path d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"/></svg> </a> </div> </div> </div> </footer> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"base": "../../..", "features": ["content.code.annotate", "content.tabs.link", "content.tooltips", "navigation.expand", "navigation.indexes", "navigation.path", "navigation.sections", "navigation.tabs", "navigation.tabs.sticky", "navigation.top", "navigation.footer", "navigation.tracking", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.b8dbb3d2.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script> <script src=../../../assets/javascripts/bundle.e1c3ead8.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> </body> </html>